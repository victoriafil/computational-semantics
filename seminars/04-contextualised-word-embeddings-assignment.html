<h1 id="s4-contextualised-word-embeddings">S4: Contextualised word
embeddings</h1>
<p>This assignment is a part of the preparation for the seminar on 12
May.</p>
<p>Please have a look <a
href="https://canvas.gu.se/courses/51974/pages/seminar-assignments-and-discussions">here</a>
for how to prepare for the seminar.</p>
<p>In this seminar we will look at contextualised word embeddings.
Previously we have learned word embeddings and language models as
separate representations but which can be trained in the same end-to-end
task. In this seminar we will look at the recent developments where both
tasks have been integrated more closely.</p>
<p>When predicting a sequence of words, a mechanism of self-attention
has been introduced which weights the importance of the previous
generated words for the prediction of the next word. Organising
attention heads over several layers allows us to learn structural
dependencies between words at different levels of abstraction.</p>
<p>The development lead to pre-training very large language models that
capture very fine grained semantic and world common-sense knowledge.
These can be then used in particular NLP tasks whose performance has
been greatly improved. However, training and using these large-scale
models comes at cost. Firstly, training requires large (and expensive)
computing infrastructure (not available to everyone) which consumes a
lot of energy. Secondly, the models are trained on texts found on the
web, but such texts do not represent all social groups equally and
therefore the models capture particular social biases.</p>
<ul>
<li>J. Devlin, M. Chang, K. Lee, and K. Toutanova. <a
href="http://arxiv.org/abs/1810.04805">BERT: pre-training of deep
bidirectional transformers for language understanding.</a> arXiv,
arXiv:1810.04805 [cs.CL]:1–14, 2018.</li>
<li>E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. <a
href="https://doi.org/10.1145/3442188.3445922">On the dangers of
stochastic parrots: Can language models be too big?</a> In Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and
Transparency, FAccT ’21, pages 610–623, New York, NY, USA, 2021.
Association for Computing Machinery.</li>
</ul>
<p>Optional further reading (quite advanced):</p>
<ul>
<li>A.Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N.
Gomez, L􏰀. Kaiser, and I. Polosukhin. <a
href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention
is all you need.</a> In Advances in Neural Information Processing
Systems, pages 5998–6008, 2017.</li>
<li>M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
L. Zettlemoyer. <a href="https://www.aclweb.org/anthology/N18-1202">Deep
contextualized word representations.</a> In Proceedings of the 2018
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers), pages 2227–2237, New Orleans, Louisiana, June 2018. Association
for Computational Linguistics.</li>
<li>A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. <a
href="https://openai.com/blog/language-unsupervised/">Improving language
understanding by generative pre-training.</a> Technical report, OpenAI,
2018.</li>
</ul>
<p>Below are some points that you may want to consider when reading
these papers.</p>
<p>For the first paper:</p>
<ul>
<li>How does BERT compare with word embeddings and a recursive language
model?</li>
<li>Word tokenisation?</li>
<li>What layers and representations should we use?</li>
</ul>
<p>For the second paper:</p>
<ul>
<li>Do you agree with the authors of the paper?</li>
<li>How can we do better NLP?</li>
</ul>
<p>The assignment is marked with complete/incomplete with a further
feedback on a 7 level scale where 4 is sufficient to complete the
assignment; 5 is good solid work; 6 is excellent work that covers most
of the assignment; and 7 is creative work that goes beyond the
assignment.</p>
