{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dc0863-44fe-4a09-ab00-439d7534c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xilini@GU.GU.SE/anaconda3/envs/compsem/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110e0b3-85a3-4298-8c6d-8054bb7b075f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3862fdd5-4c35-4bfb-af78-c7b617dc11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 k,\n",
    "                 heads = 4,\n",
    "                 mask = False):\n",
    "        \n",
    "        # keeping multiple inheritance possibilities for nn.Module\n",
    "        # need to hardwire it otherwise, e.g. nn.Module.__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        # we want every head to have the same shape\n",
    "        # e.g., 1 x 20 x (64*8) for 8 heads\n",
    "        assert k % heads == 0\n",
    "        \n",
    "        self.k, self.heads = k, heads\n",
    "        \n",
    "        # linear layers to learn transformations of the input\n",
    "        # 512 x 512\n",
    "        self.K = nn.Linear(k, k, bias=False)\n",
    "        self.Q = nn.Linear(k, k, bias=False)\n",
    "        self.V = nn.Linear(k, k, bias=False)\n",
    "        \n",
    "        # \n",
    "        self.mergeheads = nn.Linear(k, k)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,\n",
    "                x):\n",
    "        \n",
    "        # batch size, sequence size, embedding size\n",
    "        # e.g., 1 x 10 x 512\n",
    "        b, t, k = x.size()\n",
    "        h = self.heads\n",
    "        \n",
    "        # 512 x 512 X 512 x 512 -> 512 x 512\n",
    "        # here we get **full** embeddings\n",
    "        queries = self.Q(x)\n",
    "        keys = self.K(x)\n",
    "        values = self.V(x)\n",
    "        \n",
    "        # identify the dimension size of each head\n",
    "        # e.g., 512 / 8 = 64\n",
    "        s = k // h\n",
    "        \n",
    "        # reshaping vectors into the shape incl. attention heads\n",
    "        # 1 x 10 x 8 x 64\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "        \n",
    "        # computing dot products means we need to move head dimension into the batch dimension\n",
    "        # we should be able to use torch.bmm()\n",
    "        # 8 x 10 x 64\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
    "        \n",
    "        # dot product of keys and queries\n",
    "        # 8 x 10 x 64 X 8 x 64 x 10, eliminate dimension 64 because of matrix multiplication rules\n",
    "        # dot shape: 8 x 10 x 10, b * h x t x t\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        # scaling\n",
    "        # why? our initial vectors are samples from standard normal distribution with mean 0 and variance 1\n",
    "        # when we perform multiplication between vectors of size N, the variance of the result will become N\n",
    "        # it is too sparse, so we take N and use it to normalise our result\n",
    "        dot = dot / (k ** (1/2))\n",
    "\n",
    "        # transform into probabilities \n",
    "        dot = F.softmax(dot, dim = 2)\n",
    "        # now we get normalised weights / probabilities\n",
    "        \n",
    "        # 8 x 10 x 10 X 8 x 10 x 64, eliminate dimension 10\n",
    "        # result: torch.Size([1, 8, 10, 64])\n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "        \n",
    "        # get the result in the original shape, e.g. swapped heads\n",
    "        #Â torch.Size([1, 10, 512])\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
    "        \n",
    "        #embed(); raise\n",
    "        \n",
    "        # finally, return projection of all heads together\n",
    "        # why? before this step we performed head-wise operations, and now we need to learn to put them all in the same space\n",
    "        # standard step\n",
    "        return self.mergeheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c1e5f2-7ac2-4ef4-b8d6-41cee64f7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb,\n",
    "                 heads,\n",
    "                 mask,\n",
    "                 seq_length,\n",
    "                 ff_hidden_size = 8,\n",
    "                 dropout = 0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = SelfAttention(emb,\n",
    "                                       heads = heads,\n",
    "                                       mask = mask)\n",
    "        self.mask = mask\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_size * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size * emb, emb)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        x = self.dropout(x)\n",
    "        fedforward = self.ff(x)\n",
    "        x = self.norm2(fedforward + x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecac6bb1-56d2-43b3-a5e0-0f9147d685a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 512\n",
    "heads = 8\n",
    "mask = False\n",
    "seq_length = 10\n",
    "dropout = 0.5\n",
    "ff_hidden_size = 8\n",
    "\n",
    "transformer = TransformerBlock(emb_size,\n",
    "                               heads,\n",
    "                               mask,\n",
    "                               seq_length,\n",
    "                               ff_hidden_size = ff_hidden_size,\n",
    "                               dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e93e34-293b-4627-81ab-bdc4240d14d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (attention): SelfAttention(\n",
       "    (K): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (Q): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (V): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (mergeheads): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (ff): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4096, out_features=512, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbb927c-98b3-4a43-b259-7f6a09c2f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5, 10, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a21dc3-e2e2-480b-a302-054ed0e4d218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7451ca-a179-4dcc-bc00-af0b7ef3fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = transformer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856b437b-41ab-42a2-8768-b363f263dc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3bf597-0da7-436c-929d-25d69d36ceb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsem",
   "language": "python",
   "name": "compsem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
