{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Models of Meaning / Composition Functions\n",
    "\n",
    "What are we going to learn?\n",
    "\n",
    "- what is a vector space and how we can build it\n",
    "- how we can improve word representations in the vector space\n",
    "- how we can use word-based vector space to represent more complex structures, e.g. phrases\n",
    "- how we can use vector space for NLP tasks, e.g. phrase semantic similarity task\n",
    "- why and how we can reduce complexity of the vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Vector Space?\n",
    "\n",
    "*Vector space* allows us to represent meaning of each word with a vector.\n",
    "A vector is a numeric representation, e.g. `king = [0.5, 0.1, 0.8, -0.3]`. We don't really know what all of the numbers in such vectors mean, but we can try to interpret them.\n",
    "For example, consider the example below, where each word is represented with a vector consisting of different colour bars.\n",
    "Every column consists of multiple bars and each column can be thought of as a single, fixed feature.\n",
    "\n",
    "*Rows x columns is the vector space matrix (often referred to as a word embedding matrix).*\n",
    "\n",
    "Note that the intensity of the colours also differ, e.g. each bar can reach a max (very red) and a low (very blue) value. We normaly do not know what these values are, they are learned automatically in neural networks. For now, let's imagine that the highest value is +2 and the lowest is -2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"kingqueen.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could speculate about differences between vectors to understand them better:\n",
    "\n",
    "- Note that 'man' and 'boy' are similar with each other in some columns. It is also the case that 'girl' and 'woman' are similar with each other in exactly same columns. At the same time, other words are quite different from either of the four words of interest along those columns. Does it mean we found where the meaning of 'gender' is strongly encoded?\n",
    "- We can also see that 'boy' and 'girl' are similar in some columns and different from 'man' and 'woman' in the same columns. Does it mean that the idea of 'youth' or 'age' is captured in those parts of the vectors?\n",
    "- Notice how different the 'water' vector is from all other vectors across some columns. Did those dimensions learn to encode differences between human and non-human objects?\n",
    "- In some columns, 'king' and 'queen' are very similar to each other and different from other words. Did we learn the concept of 'royalty'?\n",
    "- There is a red column that crosses all words: what could be captured in this particular bar column?\n",
    "- Do you notice anything else?\n",
    "\n",
    "We use vector space to operate with words. We need to create one from the _reference corpus_.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we *generally* create a Vector Space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector spaces are created by learning from large corpora, e.g. BNC.\n",
    "The size of the corpora matters - more examples of word use in different contexts help us to capture more fine-grained meaning with even tiny differences.\n",
    "Small corpus generally might not work very well because it captures only some meaning of the words.\n",
    "\n",
    "For example, if in our corpora we have a meaning of a `bank` as a financial institute, we cannot use learned vector space to encode the meaning of a `river bank`, given that this meaning occurs in our task dataset.\n",
    "Task dataset is the dataset of words that will be encoded within our vector space. Task dataset *might* contain words, which were never present in the vector space, think `money bank` and `river bank`. It is our task to ensure that there is no such discrepancy between the target dataset and the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector space is created by looking at all uses of all words in a big corpus of texts.\n",
    "\n",
    "We need to construct vector space for our task dataset.\n",
    "The best solution would be to use the dataset used in [1], which is British National Corpus.  \n",
    "But for simplicity and purpose of this lecture, we are going to work with Gutenberg project, offered by nltk.\n",
    "\n",
    "We are going to represent meaning of each word through its context, e.g. word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/xilini/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "corpus = nltk.corpus.gutenberg.fileids()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xilini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def preprocess(s):\n",
    "    '''\n",
    "    split text into words, lowercase them, remove punctuation and stopwords,\n",
    "    we need to keep words which make sense and can be informative for our task\n",
    "    '''\n",
    "    return [w.lower().strip(string.punctuation) for w in s if w.lower() not in stopwords.words('english')]\n",
    "\n",
    "def do_word_count(corpus):\n",
    "    '''\n",
    "    let's count all words in our texts,\n",
    "    we will need this information to ignore the least frequent words,\n",
    "    why? least frequent words are typically not that informative\n",
    "    '''\n",
    "    word_count = nltk.FreqDist()\n",
    "    for filename in corpus:\n",
    "        print('reading file', filename)\n",
    "        fn_text = nltk.corpus.gutenberg.words(filename)\n",
    "        print(len(fn_text))\n",
    "        word_count.update(preprocess(fn_text))\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am making a very important test']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing our function\n",
    "preprocess(['I am making a very important test.!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file austen-emma.txt\n",
      "192427\n",
      "reading file austen-persuasion.txt\n",
      "98171\n",
      "reading file austen-sense.txt\n",
      "141576\n",
      "reading file bible-kjv.txt\n",
      "1010654\n",
      "reading file blake-poems.txt\n",
      "8354\n",
      "reading file bryant-stories.txt\n",
      "55563\n",
      "reading file burgess-busterbrown.txt\n",
      "18963\n",
      "reading file carroll-alice.txt\n",
      "34110\n",
      "reading file chesterton-ball.txt\n",
      "96996\n",
      "reading file chesterton-brown.txt\n",
      "86063\n",
      "reading file chesterton-thursday.txt\n",
      "69213\n",
      "reading file edgeworth-parents.txt\n",
      "210663\n",
      "reading file melville-moby_dick.txt\n",
      "260819\n",
      "reading file milton-paradise.txt\n",
      "96825\n",
      "reading file shakespeare-caesar.txt\n",
      "25833\n",
      "reading file shakespeare-hamlet.txt\n",
      "37360\n",
      "reading file shakespeare-macbeth.txt\n",
      "23140\n",
      "reading file whitman-leaves.txt\n",
      "154883\n"
     ]
    }
   ],
   "source": [
    "dataset = do_word_count(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(dataset, open('./reference_corpus.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "reference_corpus = pickle.load(open('./reference_corpus.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'': 422409, 'shall': 11684, 'said': 9429, 'unto': 9010, 'lord': 8590, 'thou': 6759, 'one': 6224, 'man': 5616, 'thy': 5609, 'god': 5287, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### The Task Dataset: Phrase Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is phrase similarity task proposed in [1]. We are going to use the dataset that the authors provided.\n",
    "\n",
    "The main idea is to build a model that can automatically capture differences in meanings between _phrases_.\n",
    "Phrases consist of nouns and verbs.\n",
    "\n",
    "There are always two ground-truth **reference phrases** for a single reference verb:\n",
    "\n",
    "`the fire glowed` and `the face glowed`\n",
    "\n",
    "Each reference phrase has two variations, which introduce either a **high** or **low similarity** with the original phrase:\n",
    "\n",
    "`the fire glowed` VS `the fire burned` (high) and `the fire beamed` (low)\n",
    "\n",
    "These verbs are mapped differently for the second reference phrase:\n",
    "\n",
    "`the face glowed` VS `the face beamed` (high) and `the face burned` (low)\n",
    "\n",
    "For each combination of a reference phrase and high/low similarity phrase, we have a human rating. This rating is the numeric ground-truth that will be compared against model's predictions of similarity.\n",
    "\n",
    "The **main task for the model** is to (i) encode reference phrase, (ii) encode either a high or low similarity phrase, (iii) predict the similarity score between the reference phrase and both high/low similarity phrases. The former should have a higher similarity score and the latter should be very low. We evaluate by comparing against human ratings.\n",
    "\n",
    "We need the ground-truth phrases and variations with different similarities to capture:\n",
    "1. semantics of combinations of intransitive verbs and **different** nouns\n",
    "2. differences in nouns allow us to learn about properties of specific objects as reflected in text: both face and fire can glow, but only fire burns, while only face beams.\n",
    "\n",
    "More data examples (from the paper) are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lapataexample.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: do we capture any metaphoric meaning with these constructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adopt the following terminology:\n",
    "\n",
    "**Reference phrase** is `the fire glowed` (noun + reference intransitive verb)\n",
    "\n",
    "**Landmark phrases** are two types of phrases:\n",
    "\n",
    "High similarity landmark: `the fire burned` (noun + verb from high) (this phrase is HIGHLY similar to the reference phrase)\n",
    "Low similarity landmark: `the fire beamed` (noun + verb from low) (this phrase is NOT REALLY similar to the reference phrase)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Task Dataset\n",
    "\n",
    "Since our task dataset is different from reference corpus, we need to make sure that all words from the task dataset can be found in our reference corpus.  \n",
    "Otherwise, we will not be able to build vectors based on co-occurrences simply because our words of interest have never appeared in the reference texts and our model simply does not about some of the words from the target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant verb noun landmark input hilo\n",
      "participant20 stray thought roam 7 low\n",
      "participant20 stray discussion digress 6 high\n",
      "participant20 stray eye roam 7 high\n",
      "participant20 stray child digress 1 low\n",
      "participant20 throb body pulse 5 high\n",
      "participant20 throb head shudder 2 low\n",
      "participant20 throb voice shudder 3 low\n",
      "participant20 throb vein pulse 6 high\n",
      "participant20 chatter machine click 4 high\n"
     ]
    }
   ],
   "source": [
    "# load the task dataset\n",
    "with open('./mitchell_lapata_acl08.txt', 'r') as f:\n",
    "    phrase_dataset = f.read().splitlines()\n",
    "\n",
    "for line in phrase_dataset[:10]:\n",
    "    print(line)\n",
    "    \n",
    "# get all unique words\n",
    "words = []\n",
    "for line in phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in words:\n",
    "        words.append(verb)\n",
    "    if noun not in words:\n",
    "        words.append(noun)\n",
    "    if landmark not in words:\n",
    "        words.append(landmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for the human: rate reference and landmark in terms of semantic similarity on the scale from 1 to 7, how similar they are? 1 is not similar, 7 is very similar\n",
    "\n",
    "Example item from the data:\n",
    "\n",
    "`participant20 stray discussion digress 6 high`\n",
    "\n",
    "1. participant: `participant20`, e.g. same participant has been shown multiple pairs of reference and landmark (either high or low) phrases\n",
    "2. reference phrase: combination of noun and verb columns, e.g. `discussion stray`\n",
    "3. landmark phrase: combination of noun and landmark columns, e.g. `discussion digress`\n",
    "4. input: human rating, e.g. `7`\n",
    "5. hilo: high/low, type of the landmark, e.g. `high` (for each reference phrase, the dataset should have both low and high similarity landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ricochet\n",
      "flick\n",
      "slump\n",
      "erupt\n",
      "export\n",
      "fluctuate\n"
     ]
    }
   ],
   "source": [
    "# are there any words in the task dataset which do not appear in the reference corpus?\n",
    "to_remove = []\n",
    "for w in words:\n",
    "    if w not in reference_corpus:\n",
    "        print(w)\n",
    "        to_remove.append(w)\n",
    "# if some words are not found in the reference corpus, makes sense to ignore whole phrases with such words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "# how many words does our task dataset has in general?\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why ensuring that there are no missing words is important?\n",
    "\n",
    "We typically do not have resources to pre-train word embeddings on the task dataset.\n",
    "We normally use existing pre-trained embeddings, because they encode large vector space with words in different contexts, e.g. `GoogleNews-vectors-negative300.bin`. These embeddings were created with the goal in mind of capturing wider meaning of words, so we should benefit from them. Note that the domains of your reference corpus and task dataset matter as well - to avoid discrepancies in vector spaces we want both corpora to be approximately in the same domain or cover most of the Web (more popular approach these days).\n",
    "\n",
    "If some words from your task dataset are not in the reference corpus (that is, they are also not in your vector space), you typically would look for a better, larger vector space or train your own space. In our case, we exclude non-present words, but this problem should not occur later when we work with word embeddings, because they are either trained on the task dataset or they are taken from an extremely large corpus which is less likely to not contain even a single word from a language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the task dataset\n",
    "# we are removing all phrases which contain words, which are not in the reference corpus\n",
    "\n",
    "processed_phrase_dataset = []\n",
    "for line in phrase_dataset:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb in to_remove or noun in to_remove or landmark in to_remove:\n",
    "        continue\n",
    "    processed_phrase_dataset.append(line)\n",
    "\n",
    "target_words = []\n",
    "for line in processed_phrase_dataset[1:]:\n",
    "    _, verb, noun, landmark, _, _ = line.split()\n",
    "    if verb not in target_words:\n",
    "        target_words.append(verb)\n",
    "    if noun not in target_words:\n",
    "        target_words.append(noun)\n",
    "    if landmark not in target_words:\n",
    "        target_words.append(landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words do we have after pre-processing\n",
    "len(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our task dataset (phrase similarity dataset) matches our vector space: all words in the task dataset occur in the vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Vector Space for the Target Dataset\n",
    "\n",
    "We are going to take every target word and compute their embeddings based on frequency of the neighbouring words as defined by the context window.\n",
    "\n",
    "Often one would use the task dataset itself to compute such embeddings, but our phrases are in isolation, so, once again, that is the reason why we need reference corpus, where phrases are contextualised, e.g. exist in some context of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "# transforming words in all texts into a single string, e.g. we want to later extract context of target words from this string\n",
    "\n",
    "texts_as_words = []\n",
    "for filename in corpus:\n",
    "    fn_text = nltk.corpus.gutenberg.words(filename)\n",
    "    print(filename)\n",
    "    fn_text = [w.lower().strip(string.punctuation) for w in fn_text] #if w.lower() not in stopwords.words('english')]\n",
    "    fn_text = [w for w in fn_text if w != '']\n",
    "    texts_as_words.append(fn_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "texts_as_words = list(itertools.chain.from_iterable(texts_as_words))\n",
    "\n",
    "with open('./texts-as-words.json', 'w') as f:\n",
    "    json.dump(texts_as_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./texts-as-words.json', 'r') as f1:\n",
    "    texts_as_words = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def compute_space(window_size, corpus):\n",
    "    '''\n",
    "    this function builds vector space for each word in the task dataset\n",
    "    the space is limited by the window_size : how many words on both sides of the target word we should use to encode it?\n",
    "    the space is counting frequency of context words, e.g. the idea of co-occurence\n",
    "    '''\n",
    "    \n",
    "    space = nltk.ConditionalFreqDist()\n",
    "\n",
    "    for index in tqdm.tqdm(range(len(corpus))):\n",
    "        # current word\n",
    "        current = corpus[index]\n",
    "\n",
    "        if current in target_words:\n",
    "                                    \n",
    "            # get future context for the first word only\n",
    "            # because there is no past context for the first word\n",
    "            if index == 0:\n",
    "                for cxword_index_after in range(window_size):\n",
    "                    cxword_after = corpus[cxword_index_after + 1]\n",
    "                    space[current].update([cxword_after])      \n",
    "                    \n",
    "            # context before and after the current word: specified by context size\n",
    "            if index > 0:\n",
    "                \n",
    "                # extract words within the context window for the current word that occur BEFORE\n",
    "                for cxword_index_before in range(max(index - window_size, 0), index):\n",
    "                    # range is inclsuive the first value but exclusive the second\n",
    "                    cxword_before = corpus[cxword_index_before]\n",
    "                    # In a ConditionalFreqDist, if 'current' is not a condition yet,\n",
    "                    # then accessing it creates a new empty FreqDist for 'current'\n",
    "                    # The FreqDist method inc() increments the count for the given item by one.\n",
    "                    space[current].update([cxword_before])\n",
    "\n",
    "                # extract words within the context window for the current word that occur AFTER\n",
    "                if index + window_size < len(corpus):\n",
    "                    for cxword_index_after in range(index, max(index + window_size, 0)):\n",
    "                        cxword_after = corpus[cxword_index_after + 1]\n",
    "                        space[current].update([cxword_after])\n",
    "\n",
    "                # if window_size AFTER exceeds the length of the corpus (needed for the words in the end)\n",
    "                else:\n",
    "                    for cxword_index_after in range(index + 1, len(corpus)):\n",
    "                        cxword_after = corpus[cxword_index_after]\n",
    "                        space[current].update([cxword_after])\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 2199204/2199204 [00:01<00:00, 1107315.55it/s]\n"
     ]
    }
   ],
   "source": [
    "sp = compute_space(5, texts_as_words)\n",
    "pickle.dump(sp, open('./vector-space-freq.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 80 conditions>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stray',\n",
       " 'thought',\n",
       " 'roam',\n",
       " 'discussion',\n",
       " 'digress',\n",
       " 'eye',\n",
       " 'child',\n",
       " 'throb',\n",
       " 'body',\n",
       " 'pulse']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('the', 11), ('to', 6), ('from', 5), ('a', 4), ('of', 4), ('and', 4), ('or', 3), ('near', 3), ('me', 3), ('letter', 2)] \n",
      "\n",
      "thought:\n",
      " [('i', 543), ('the', 506), ('of', 444), ('and', 365), ('he', 342), ('to', 326), ('it', 302), ('a', 261), ('that', 245), ('she', 215)] \n",
      "\n",
      "discussion:\n",
      " [('of', 14), ('the', 12), ('and', 10), ('a', 6), ('in', 6), ('to', 5), ('one', 4), ('their', 4), ('an', 3), ('s', 3)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp['stray'].most_common(10), '\\n')\n",
    "print('thought:\\n', sp['thought'].most_common(10), '\\n')\n",
    "print('discussion:\\n', sp['discussion'].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem**: the size of the vectors might impact our representations. What happens if we take bigger sizes for our word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('the', 11), ('to', 6), ('from', 5), ('a', 4), ('of', 4), ('and', 4), ('or', 3), ('near', 3), ('me', 3), ('letter', 2), ('as', 2), ('up', 2), ('him', 2), ('might', 2), ('s', 2), ('into', 2), ('you', 2), ('out', 2), ('side', 2), ('with', 2), ('returning', 1), ('exercise', 1), ('boy', 1), ('on', 1), ('an', 1), ('obstinate', 1), ('he', 1), ('took', 1), ('any', 1), ('how', 1), ('beautifully', 1), ('wish', 1), ('church', 1), ('then', 1), ('parson', 1), ('preach', 1), ('car', 1), ('like', 1), ('cat', 1), ('left', 1), ('swaying', 1), ('at', 1), ('front', 1), ('door', 1), ('revellers', 1), ('if', 1), ('there', 1), ('half', 1), ('almost', 1), ('complete', 1), ('solitude', 1), ('they', 1), ('could', 1), ('so', 1), ('only', 1), ('four', 1), ('finally', 1), ('last', 1), ('merry', 1), ('maker', 1), ('ran', 1), ('not', 1), ('understand', 1), ('let', 1), ('little', 1), ('too', 1), ('lest', 1), ('guinea', 1), ('hen', 1), ('fall', 1), ('again', 1), ('purpose', 1), ('popping', 1), ('off', 1), ('narwhales', 1), ('vagrant', 1), ('sea', 1), ('unicorns', 1), ('whenever', 1), ('oar', 1), ('bit', 1), ('plank', 1), ('through', 1), ('groves', 1), ('coral', 1), ('sporting', 1), ('quick', 1), ('glance', 1), ('thy', 1), ('henceforth', 1), ('where', 1), ('er', 1), ('our', 1), ('day', 1), ('them', 1), ('that', 1), ('would', 1), ('pedler', 1), ('sweats', 1), ('his', 1)] \n",
      "\n",
      "thought:\n",
      " [('i', 543), ('the', 506), ('of', 444), ('and', 365), ('he', 342), ('to', 326), ('it', 302), ('a', 261), ('that', 245), ('she', 215), ('was', 208), ('you', 181), ('in', 160), ('have', 143), ('but', 136), ('had', 135), ('as', 130), ('be', 128), ('would', 118), ('for', 111), ('her', 104), ('so', 98), ('at', 95), ('his', 94), ('him', 91), ('not', 87), ('with', 84), ('this', 71), ('is', 69), ('all', 69), ('s', 67), ('what', 66), ('they', 65), ('could', 62), ('never', 58), ('me', 57), ('very', 56), ('my', 52), ('no', 51), ('which', 50), ('if', 49), ('one', 48), ('should', 47), ('were', 45), ('we', 44), ('when', 44), ('them', 44), ('alice', 44), ('said', 43), ('by', 42), ('or', 39), ('must', 38), ('on', 38), ('might', 37), ('from', 36), ('little', 36), ('well', 35), ('now', 35), ('been', 34), ('first', 33), ('only', 33), ('some', 33), ('then', 33), ('good', 32), ('than', 32), ('any', 31), ('how', 31), ('more', 31), ('there', 30), ('mr', 29), ('herself', 29), ('man', 29), ('about', 29), ('an', 28), ('who', 27), ('thing', 27), ('emma', 26), ('do', 26), ('much', 25), ('such', 24), ('old', 24), ('every', 23), ('their', 23), ('time', 23), ('your', 22), ('always', 22), ('yet', 22), ('ever', 21), ('too', 21), ('come', 21), ('shall', 21), ('out', 20), ('himself', 20), ('say', 19), ('because', 19), ('most', 18), ('mrs', 18), ('almost', 18), ('long', 18), ('god', 18)] \n",
      "\n",
      "discussion:\n",
      " [('of', 14), ('the', 12), ('and', 10), ('a', 6), ('in', 6), ('to', 5), ('one', 4), ('their', 4), ('an', 3), ('s', 3), ('me', 2), ('as', 2), ('his', 2), ('were', 2), ('such', 2), ('that', 2), ('had', 2), ('her', 2), ('they', 2), ('was', 2), ('only', 2), ('it', 2), ('made', 1), ('matter', 1), ('much', 1), ('among', 1), ('you', 1), ('pray', 1), ('excuse', 1), ('nothing', 1), ('so', 1), ('interesting', 1), ('concerns', 1), ('every', 1), ('friendship', 1), ('long', 1), ('under', 1), ('them', 1), ('succeeded', 1), ('open', 1), ('frequent', 1), ('hopes', 1), ('chances', 1), ('she', 1), ('conviction', 1), ('all', 1), ('farther', 1), ('confidential', 1), ('topic', 1), ('better', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), ('evening', 1), ('indulgence', 1), ('subjects', 1), ('which', 1), ('usual', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), ('shyness', 1), ('nor', 1), ('reserve', 1), ('speedily', 1), ('discovered', 1), ('earned', 1), ('privilege', 1), ('intimate', 1), ('sister', 1), ('disappointment', 1), ('preparing', 1), ('marianne', 1), ('for', 1), ('its', 1), ('no', 1), ('time', 1), ('be', 1), ('unpleasant', 1), ('subject', 1), ('may', 1), ('k', 1), ('chesterton', 1), ('1909', 1), ('i', 1), ('somewhat', 1), ('air', 1), ('cab', 1), ('iv', 1), ('at', 1), ('dawn', 1), ('duellists', 1), ('really', 1), ('important', 1), ('part', 1), ('macian', 1), ('wore', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp['stray'].most_common(100), '\\n')\n",
    "print('thought:\\n', sp['thought'].most_common(100), '\\n')\n",
    "print('discussion:\\n', sp['discussion'].most_common(100), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove stop words since they are not adding anything useful to our representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding stopwords from the semantic space...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1088907/1088907 [00:01<00:00, 943892.89it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in texts_as_words if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "print('excluding stopwords from the semantic space...')\n",
    "\n",
    "sp2 = compute_space(5, filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sp2, open('./vector-space-freq-2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp2 = pickle.load(open('./vector-space-freq-2.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('near', 3), ('side', 3), ('letter', 2), ('little', 2), ('day', 2), ('might', 2), ('back', 2), ('let', 2), ('hour', 2), ('garden', 2)] \n",
      "\n",
      "thought:\n",
      " [('would', 188), ('said', 124), ('could', 123), ('one', 106), ('never', 99), ('little', 80), ('man', 67), ('good', 66), ('thought', 66), ('must', 65)] \n",
      "\n",
      "discussion:\n",
      " [('one', 4), ('every', 2), ('must', 2), ('least', 2), ('given', 2), ('could', 2), ('would', 2), ('book', 2), ('smith', 1), ('intimacy', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp2['stray'].most_common(10), '\\n')\n",
    "print('thought:\\n', sp2['thought'].most_common(10), '\\n')\n",
    "print('discussion:\\n', sp2['discussion'].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about punctuation? We can also remove punctuation from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding punctuation from the semantic space...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1088907/1088907 [00:01<00:00, 944789.71it/s]\n"
     ]
    }
   ],
   "source": [
    "real_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "print('excluding punctuation from the semantic space...')\n",
    "\n",
    "sp3 = compute_space(5, real_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sp3, open('./vector-space-freq-3.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3 = pickle.load(open('./vector-space-freq-3.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray:\n",
      " [('near', 3), ('side', 3), ('letter', 2), ('little', 2), ('day', 2), ('might', 2), ('back', 2), ('let', 2), ('hour', 2), ('garden', 2), ('hands', 2), ('sea', 2), ('sun', 2), ('cole', 1), ('carriage', 1), ('horses', 1), ('returning', 1), ('exercise', 1), ('boy', 1), ('obstinate', 1), ('mule', 1), ('liveliest', 1), ('boys', 1), ('fondly', 1), ('pointing', 1), ('took', 1), ('beautifully', 1), ('emma', 1), ('written', 1), ('livelong', 1), ('ever', 1), ('wish', 1), ('church', 1), ('parson', 1), ('preach', 1), ('drink', 1), ('sing', 1), ('aspiring', 1), ('idiot', 1), ('car', 1), ('like', 1), ('cat', 1), ('left', 1), ('swaying', 1), ('unsteadily', 1), ('moon', 1), ('merely', 1), ('us', 1), ('front', 1), ('door', 1), ('revellers', 1), ('half', 1), ('inquiry', 1), ('cooked', 1), ('sense', 1), ('almost', 1), ('complete', 1), ('solitude', 1), ('could', 1), ('wall', 1), ('apparently', 1), ('unwatched', 1), ('soon', 1), ('four', 1), ('finally', 1), ('last', 1), ('merry', 1), ('maker', 1), ('ran', 1), ('house', 1), ('whooping', 1), ('professor', 1), ('head', 1), ('understand', 1), ('hell', 1), ('gogol', 1), ('said', 1), ('fast', 1), ('lest', 1), ('guinea', 1), ('hen', 1), ('fall', 1), ('enemy', 1), ('miss', 1), ('barbara', 1), ('powder', 1), ('flask', 1), ('shot', 1), ('purpose', 1), ('popping', 1), ('narwhales', 1), ('vagrant', 1), ('unicorns', 1), ('infesting', 1), ('feeling', 1), ('flukes', 1), ('whenever', 1), ('oar', 1), ('bit', 1), ('plank', 1)] \n",
      "\n",
      "thought:\n",
      " [('would', 188), ('said', 124), ('could', 123), ('one', 106), ('never', 99), ('little', 80), ('man', 67), ('good', 66), ('thought', 66), ('must', 65), ('much', 63), ('well', 62), ('time', 60), ('mr', 58), ('might', 54), ('alice', 52), ('shall', 51), ('know', 50), ('like', 49), ('say', 48), ('thing', 48), ('first', 47), ('come', 43), ('see', 42), ('every', 40), ('mrs', 40), ('old', 39), ('upon', 38), ('always', 37), ('yet', 37), ('great', 36), ('day', 35), ('think', 35), ('ever', 34), ('father', 34), ('way', 34), ('emma', 33), ('go', 33), ('sure', 33), ('looked', 33), ('nothing', 33), ('long', 33), ('though', 32), ('miss', 31), ('make', 31), ('quite', 30), ('saw', 30), ('even', 30), ('went', 29), ('last', 29), ('take', 29), ('god', 29), ('give', 28), ('perhaps', 28), ('us', 28), ('made', 27), ('cried', 27), ('heard', 27), ('love', 26), ('right', 26), ('thee', 26), ('knew', 25), ('came', 25), ('almost', 25), ('indeed', 25), ('poor', 25), ('unto', 25), ('done', 24), ('least', 24), ('dear', 24), ('thy', 24), ('better', 23), ('may', 22), ('two', 22), ('death', 22), ('ye', 22), ('away', 21), ('world', 21), ('yes', 21), ('best', 21), ('something', 21), ('mind', 21), ('another', 21), ('people', 20), ('enough', 20), ('harriet', 20), ('room', 20), ('mother', 20), ('house', 20), ('soon', 20), ('life', 20), ('things', 20), ('told', 20), ('thou', 20), ('really', 19), ('young', 19), ('look', 19), ('possible', 19), ('seen', 19), ('men', 19)] \n",
      "\n",
      "discussion:\n",
      " [('one', 4), ('every', 2), ('must', 2), ('least', 2), ('given', 2), ('could', 2), ('would', 2), ('book', 2), ('smith', 1), ('intimacy', 1), ('made', 1), ('matter', 1), ('much', 1), ('among', 1), ('pray', 1), ('excuse', 1), ('supposing', 1), ('little', 1), ('mr', 1), ('elton', 1), ('found', 1), ('nothing', 1), ('interesting', 1), ('concerns', 1), ('report', 1), ('therefore', 1), ('cold', 1), ('fetching', 1), ('letters', 1), ('friendship', 1), ('long', 1), ('succeeded', 1), ('equally', 1), ('thing', 1), ('like', 1), ('unreserve', 1), ('open', 1), ('frequent', 1), ('hopes', 1), ('chances', 1), ('perfectly', 1), ('resolved', 1), ('believed', 1), ('hartfield', 1), ('acknowledging', 1), ('conviction', 1), ('farther', 1), ('confidential', 1), ('topic', 1), ('better', 1), ('avoided', 1), ('hoping', 1), ('harden', 1), ('nerves', 1), ('sufficiently', 1), ('feel', 1), ('continual', 1), ('crofts', 1), ('business', 1), ('evil', 1), ('assisted', 1), ('however', 1), ('persuasion', 1), ('evening', 1), ('indulgence', 1), ('subjects', 1), ('usual', 1), ('companions', 1), ('probably', 1), ('concern', 1), ('meet', 1), ('even', 1), ('lady', 1), ('russell', 1), ('merits', 1), ('anne', 1), ('understand', 1), ('points', 1), ('introduced', 1), ('neither', 1), ('shyness', 1), ('reserve', 1), ('speedily', 1), ('discovered', 1), ('enjoyment', 1), ('dancing', 1), ('music', 1), ('dashwood', 1), ('abundantly', 1), ('earned', 1), ('privilege', 1), ('intimate', 1), ('sister', 1), ('disappointment', 1), ('friendly', 1), ('zeal', 1), ('endeavoured', 1), ('soon', 1), ('saw', 1), ('necessity', 1)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('stray:\\n', sp3['stray'].most_common(100), '\\n')\n",
    "print('thought:\\n', sp3['thought'].most_common(100), '\\n')\n",
    "print('discussion:\\n', sp3['discussion'].most_common(100), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we achieve?**\n",
    "\n",
    "We have represented each word in our task dataset by the context that it keeps in our reference corpus.\n",
    "The context differs from word to word and columns (words) do not necessarily overlap.\n",
    "We also keep frequency of words to represent it as a numeric vector later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final processing touches\n",
    "\n",
    "Lastly, we need to have a single context for all words.\n",
    "We are going to iterate through our target words and take words from every context and blend them into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space_task_dataset = pickle.load(open('./vector-space-freq-3.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {}\n",
    "for item in target_words:\n",
    "    # similar to [1],\n",
    "    # we identify the most frequent co-occurring words for all target words\n",
    "    # we need to take those, which seem to be appearing very often in different words' context\n",
    "    fixed_sem_space = dict(vector_space_task_dataset[item])#.most_common(2000))\n",
    "    \n",
    "    for word, freq in fixed_sem_space.items():\n",
    "        if word not in columns:\n",
    "            columns[word] = freq\n",
    "        else:\n",
    "            if columns[word] < freq:\n",
    "                columns[word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(columns.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 814), ('shall', 776), ('said', 613), ('unto', 582), ('every', 498)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19646"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will decide on the size of our vectors.\n",
    "We are going to take the words from the top of the sorted list, because they are the most frequent ones and they are likely to appear in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every target word will be represented through these dimensions (most frequent and sensible words)\n",
    "dims = dict(sorted_columns[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our final vector space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 80/80 [00:00<00:00, 460.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# building our final vector space for our target words\n",
    "target_semantic_space = {}\n",
    "for item in tqdm.tqdm(target_words):\n",
    "    target_semantic_space[item] = {}\n",
    "    this_sem_space = dict(vector_space_task_dataset[item])\n",
    "    # we are going to use context words ONLY, and then take frequency from our vector space\n",
    "    for other_w in dims.keys():\n",
    "        if other_w not in this_sem_space.keys():\n",
    "            target_semantic_space[item][other_w] = [0]\n",
    "        else:\n",
    "            target_semantic_space[item][other_w] = [this_sem_space[other_w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   man  shall  said  unto  every  god  hand  lord  one  thou  ...  ivory  \\\n",
      "0  111    114   122    71     30   60    33    88   69    76  ...      1   \n",
      "\n",
      "   jonathan  furnace  mock  anarchist  incline  cannon  froward  noah  cherub  \n",
      "0         1        1     1          1        1       1        1     1       6  \n",
      "\n",
      "[1 rows x 1455 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(target_semantic_space['face'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   man  shall  said  unto  every  hand  lord  one  thou  old  ...  parchment  \\\n",
      "0    7     37     8     1      2     3     2    3     6    4  ...          1   \n",
      "\n",
      "   complexion  serpent  peculiar  blubber  bone  whereof  hoisted  beating  \\\n",
      "0           3        1         1        6     2        1        1        1   \n",
      "\n",
      "   shaven  \n",
      "0       1  \n",
      "\n",
      "[1 rows x 384 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(target_semantic_space['skin'])\n",
    "m2 = (df != 0).all()\n",
    "print(df.loc[:, m2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./target_semantic_space.json', 'w') as f2:\n",
    "    json.dump(target_semantic_space, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./target_semantic_space.json', 'r') as f3:\n",
    "    our_space = json.load(f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Similarity Task\n",
    "\n",
    "We want to make a model that is able to see the differences between phrases / sentences, not solely individual words.  \n",
    "Then, we will need to test this model and check whether its performance correlates with human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "\n",
    "We need to decide how we are going to combine representations from words into phrase representations.\n",
    "Since we have every word represented through counts of words from its context, we can do some operations as exemplified below (from [1]):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have two vectors: one is for a noun, another one is for a verb.\n",
    "\n",
    "u = `[15, 16, 5, 6]` (stands for _house_, vector size / number of dimensions = 4)\n",
    "\n",
    "v = `[4, 5, 3, 4]` (stands for _burn_)\n",
    "\n",
    "What are the composition functions we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: `house + burn = house burn`\n",
    "\n",
    "![title](simple_additive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: `house * burn = house burn`\n",
    "\n",
    "![title](simple_multiplicative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: multiplication and addition combined\n",
    "\n",
    "![title](combined.png)\n",
    "\n",
    "Alpha, beta, gamma: three variables, which control how much each constituent contributes to the result\n",
    "\n",
    "If alpha is `0.0`, it means that no contribution will be given by `u`.\n",
    "\n",
    "We will set `alpha` = 0, `beta` = 0.95, `gamma` = 0.05 (based on [1]).\n",
    "It means that for the phrase `house burn`, if we go with the last method we are saying that meaning of `house` is not needed, while it is the meaning of the verb that will contribute the most. The meaning of multiplication of noun and verb is also needed to some extent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test these representations and see how different they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant50 chatter child gabble 6 high',\n",
       " 'participant50 chatter tooth click 2 high',\n",
       " 'participant50 reel head whirl 5 high',\n",
       " 'participant50 reel mind stagger 4 low',\n",
       " 'participant50 reel industry stagger 5 high',\n",
       " 'participant50 reel man whirl 3 low',\n",
       " 'participant50 glow fire beam 7 low',\n",
       " 'participant50 glow face burn 3 low',\n",
       " 'participant50 glow cigar burn 5 high',\n",
       " 'participant50 glow skin beam 7 high']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_phrase_dataset[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['participant', 'verb', 'noun', 'landmark', 'input', 'hilo']\n"
     ]
    }
   ],
   "source": [
    "# the first line is the column name line, we ignore it\n",
    "column_names = phrase_dataset[0].split()\n",
    "print(column_names)\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for line in phrase_dataset[1:]:\n",
    "    participant_id, reference, noun, landmark, rating, hilo = line.split()\n",
    "    reference_phrase = [noun, reference]\n",
    "    landmark_phrase = [noun, landmark]\n",
    "\n",
    "    if participant_id not in dataset:\n",
    "        dataset[participant_id] = []\n",
    "    else:\n",
    "        dataset[participant_id].append((reference_phrase, landmark_phrase, rating, hilo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['discussion', 'stray'], ['discussion', 'digress'], '6', 'high'),\n",
       " (['eye', 'stray'], ['eye', 'roam'], '1', 'high'),\n",
       " (['child', 'stray'], ['child', 'digress'], '1', 'low'),\n",
       " (['head', 'reel'], ['head', 'stagger'], '4', 'low'),\n",
       " (['mind', 'reel'], ['mind', 'whirl'], '5', 'high'),\n",
       " (['industry', 'reel'], ['industry', 'whirl'], '2', 'low'),\n",
       " (['man', 'reel'], ['man', 'stagger'], '5', 'high'),\n",
       " (['cigarette', 'flare'], ['cigarette', 'erupt'], '1', 'low'),\n",
       " (['eye', 'flare'], ['eye', 'flame'], '2', 'high'),\n",
       " (['argument', 'flare'], ['argument', 'erupt'], '6', 'high')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['participant1'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Computing Phrase Similarity\n",
    "\n",
    "One of the primary methods in which distributional vectors are used is to estimate similarity between words.\n",
    "\n",
    "The central idea is that words that appear in similar contexts tend to be similar in meaning.\n",
    "So what we need to do to estimate word similarity from distributional vectors is to compute a similarity measure that determines how similar the vectors of two words are.\n",
    "\n",
    "There are many similarity measures, but the one that is used the most is **cosine similarity**.\n",
    "If we consider the vector of a word as an arrow from the origin, then two words should be similar if their vectors go in roughly the same direction.\n",
    "Cosine similarity measures this as the cosine of the angle between the two vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cosine(u, v) = \\frac{\\sum_i {u_i v_i}}{|u| |v|}$$\n",
    "\n",
    "$$|v| = \\sqrt{\\sum_i v_i^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why cosine?\n",
    "\n",
    "The similarity metric between vectors is typically based on the dot product:\n",
    "\n",
    "<img src=\"dotproduct.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "However, dot products get bigger if the vectors are longer; the vectors are longer if they have higher values in each dimension.\n",
    "That is, more frequent words will have higher dot products; we do not want a similarity metric that is sensitic to word frequency.\n",
    "\n",
    "Solution: divide the dot product by the length of the two vectors, which also appears to be a cosine formula for the angle between both vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# similarity measure: cosine\n",
    "#                           sum_i vec1_i * vec2_i\n",
    "# cosine(vec1, vec2) = ------------------------------\n",
    "#                        veclen(vec1) * veclen(vec2)\n",
    "# where\n",
    "#\n",
    "# veclen(vec) = squareroot( sum_i vec_i*vec_i )\n",
    "#\n",
    "\n",
    "def veclen(vector):\n",
    "    return math.sqrt(np.sum(np.square(vector)))\n",
    "\n",
    "def cosine(vector1, vector2):\n",
    "    veclen1 = veclen(vector1)\n",
    "    veclen2 = veclen(vector2)\n",
    "    if veclen1 == 0.0 or veclen2 == 0.0:\n",
    "        # one of the vectors is empty, the cosine is 0\n",
    "        return 0.0\n",
    "    else:\n",
    "        # we could also simply do:\n",
    "        dotproduct = np.dot(vector1, vector2)\n",
    "        #print(vector1, vector2)\n",
    "        return dotproduct / (veclen1 * veclen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_space(phrase, x_names):\n",
    "\n",
    "    # first we get representations for verb and noun\n",
    "    subject_space = target_semantic_space[phrase[0]]\n",
    "    verb_space = target_semantic_space[phrase[1]]\n",
    "    # representation for house and burn\n",
    "    \n",
    "    representation = np.zeros(len(x_names))\n",
    "\n",
    "    for n, word in enumerate(x_names.keys()):\n",
    "\n",
    "        # I get v^Ith element from each of the vectors\n",
    "        subject_value = subject_space[word][0]\n",
    "        verb_value = verb_space[word][0]\n",
    "\n",
    "        #out = subject_value + verb_value\n",
    "        out = subject_value * verb_value\n",
    "        \n",
    "        # 6 and 0, if we do summation, we are getting 6\n",
    "        # if we do mulitplication, we are getting 0\n",
    "        \n",
    "        #out = subject_value * 0.2 + verb_value * 0.8\n",
    "        #out = subject_value * 0.0 + verb_value * 0.95 + (0.05 * subject_value * verb_value)\n",
    "\n",
    "        representation[n] = out\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111. 114. 610. ...   0.   0.   0.] (2000,)\n",
      "[222. 342. 244. ...   0.   0.   0.]\n",
      "[  444. 10602.   610. ...     0.     0.     0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "reference = ['face', 'glow']\n",
    "landmark_high = ['face', 'beam']\n",
    "landmark_low = ['face', 'burn']\n",
    "\n",
    "ref = build_phrase_space(reference, dims)\n",
    "\n",
    "lhigh = build_phrase_space(landmark_high, dims)\n",
    "\n",
    "llow = build_phrase_space(landmark_low, dims)\n",
    "\n",
    "print(ref, ref.shape)\n",
    "print(lhigh)\n",
    "print(llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4153545549382808"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, lhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19922629518346402"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, llow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Transforming counts to association weights\n",
    "\n",
    "So far we have been representing words through vectors that include frequency of context / neighbouring words.\n",
    "However, one problem is that all words will have high co-occurrence counts with the most frequent context items.\n",
    "These words become less informative for the model to learn from, because they appear so frequently across the contexts.\n",
    "Some examples of such words are as follows: i, a, on, with, the, man, could, etc.\n",
    "\n",
    "Keeping these words and their frequencies will falsely inflate all our similarity estimates.\n",
    "What we want to know instead is how strongly a target word is associated with a context item: does it appear with the context item more often than we could expect at random? Less often? About as often as we would expect?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple options for computing degree of association:\n",
    "\n",
    "- tf-idf (term frequency / inverse document frequency)\n",
    "- pointwise mutual information (PMI)\n",
    "- positive mutual information (PPMI): just change negative PMI values to zero\n",
    "- local mutual information (LMI)\n",
    "\n",
    "We do PMI here.\n",
    "The PMI of a target word t and context item c is defined as:\n",
    " \n",
    "$$PMI(t, c) = log \\frac{P(t, c)}{P(t) P(c)}$$\n",
    "\n",
    "All the probabilities are computed from the table of counts. We need:\n",
    "\n",
    "- $\\#(t, c)$: the co-occurrence count of t with c\n",
    "- $\\#(\\_, \\_)$: the sum of counts in the whole table, across all targets\n",
    "- $\\#(t, \\_)$: the sum of counts in the row of target t\n",
    "- $\\#(\\_, c)$: the sum of counts in the column of context item c\n",
    "\n",
    "Then we have: \n",
    "\n",
    "- $P(t, c) = \\frac{\\#(t, c)}{\\#(\\_,\\_)}$\n",
    "- $P(t) = \\frac{\\#(t,\\_)}{\\#(\\_,\\_)}$\n",
    "- $P(c) = \\frac{\\#(\\_,c)}{\\#(\\_,\\_)}$\n",
    "\n",
    "Here is how we compute PPMI from counts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ppmi.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# transform the space using positive pointwise mutual information\n",
    "\n",
    "# target t, dimension value c, then\n",
    "# PMI(t, c) = log ( P(t, c) / (P(t) P(c)) )\n",
    "# where\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) =   PMI(t, c) if PMI(t, c) > 0\n",
    "#                0 else\n",
    "\n",
    "def ppmi_transform(space, word):\n",
    "    \n",
    "    row_sums = {}\n",
    "    col_sums = {}\n",
    "    context_word = {}\n",
    "    \n",
    "    pmi_return = {}\n",
    "    \n",
    "    #(_, _): overall count of occurrences\n",
    "    overall = 0\n",
    "    for _, vectors in space.items():\n",
    "        for _, f in vectors.items():\n",
    "            overall += f[0]    \n",
    "    \n",
    "    for t, vectors in space.items():\n",
    "        \n",
    "        if t == word:\n",
    "            # #(t, _): for each target word, sum up all its counts.\n",
    "            # row_sums is a dictionary mapping from target words to row sums\n",
    "            # how many time t appears in the context\n",
    "            t_sum = 0\n",
    "            for _, f in vectors.items():\n",
    "                t_sum += f[0]\n",
    "            row_sums[t] = t_sum\n",
    "            \n",
    "        # #(_, c): for each context word, sum up all its counts\n",
    "        # col_sums is a dictionary mapping from context word indices to column sums\n",
    "        #col_sums = {}\n",
    "        #for c, f in vectors.items():\n",
    "        #    col_sums[c] = sum([elem[0] for elem in f])\n",
    "        #print(col_sums)\n",
    "        \n",
    "        # #(_, c): for each context word, sum up all its counts\n",
    "        # col_sums is a dictionary mapping from context word indices to column sums\n",
    "        for c, f in vectors.items():\n",
    "            if c not in col_sums:\n",
    "                col_sums[c] = f[0]\n",
    "            else:\n",
    "                col_sums[c] += f[0]\n",
    "\n",
    "    #print(col_sums)\n",
    "    #print(row_sums)\n",
    "    #print(context_word)\n",
    "    pmi_return[word] = {}\n",
    "    \n",
    "    for context_word, context_sums in col_sums.items():\n",
    "        target_pmi = np.log2((context_sums / overall) / (row_sums[word] / overall) * (col_sums[context_word] / overall))\n",
    "\n",
    "        pmi_return[word][context_word] = target_pmi\n",
    "        \n",
    "    return pmi_return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_semantic_space['stray']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stray\n",
      "-2.361634103986254 man\n",
      "-1.0860896287313941 shall\n",
      "-1.6253318806902384 said\n",
      "-2.7841378314487493 unto\n",
      "-3.6117477379455405 every\n"
     ]
    }
   ],
   "source": [
    "for w in target_words:\n",
    "    \n",
    "    ppmispace = ppmi_transform(target_semantic_space, w)\n",
    "    \n",
    "    for k, v in ppmispace.items():\n",
    "        print(k)\n",
    "        for c in list(v)[:5]:\n",
    "            print(v[c], c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is a method that takes a space where each word has a vector of, say, 10,000 dimensions and reduces it to a space where each word has a vector of something like 300 or 500 dimensions, making the space more manageable.\n",
    "\n",
    "The new dimensions can be seen as groupings (soft clusterings) of the old dimensions, or as latent semantic classes underlying the old dimensions. A popular choice of dimensionality reduction method is _singular value decomposition (SVD)_. SVD involves representing a set of points in a different space (that is, through a new set of dimensions) in such a way that it brings out the underlying structure of the data.\n",
    "\n",
    "Here is how we can do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_semantic_space['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to make a copy of our original vector space with frequencies, which will be later pruned\n",
    "space_to_reduce = {}\n",
    "for w in target_words:\n",
    "    \n",
    "    space_to_reduce[w] = np.zeros(2000)\n",
    "    this_sem_space = dict(target_semantic_space[w])\n",
    "    \n",
    "    for n, other_w in enumerate(dims.keys()):\n",
    "        if other_w not in this_sem_space.keys():\n",
    "            space_to_reduce[w][n] = 0\n",
    "        else:\n",
    "            space_to_reduce[w][n] = dims[other_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#space_to_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_transform(space, original_dim, dim_to_keep):\n",
    "    \n",
    "    # space is a dictionary mapping words to vectors\n",
    "    # combine those into a big matrix\n",
    "    spacematrix = np.empty((len(space.keys()), original_dim))\n",
    "    rowlabels = sorted(space.keys())\n",
    "\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        spacematrix[index] = space[word]\n",
    "\n",
    "    # start SVD\n",
    "    umatrix, sigmavector, vmatrix = np.linalg.svd(spacematrix)\n",
    "\n",
    "    \n",
    "    # remove the last few dimensions of u and sigma\n",
    "    utrunc = umatrix[:, :dim_to_keep]\n",
    "    sigmatrunc = sigmavector[ :dim_to_keep]\n",
    "\n",
    "    # new space: U %matrixproduct% Sigma_as_diagonal_matrix   \n",
    "    newspacematrix = np.dot(utrunc, np.diag(sigmatrunc))\n",
    "\n",
    "    # transform back to a dictionary mapping words to vectors\n",
    "    newspace = {}\n",
    "    for index, word in enumerate(rowlabels):\n",
    "        newspace[word] = newspacematrix[index]\n",
    "        \n",
    "    return newspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_space = svd_transform(space_to_reduce, 2000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Now, let us inspect the space a bit more\n",
    "\n",
    "space = space_to_reduce\n",
    "original_dim = 2000\n",
    "dim_to_keep = 50\n",
    "\n",
    "spacematrix = np.empty((len(space.keys()), original_dim))\n",
    "print(spacematrix.shape)\n",
    "rowlabels = sorted(space.keys())\n",
    "\n",
    "for index, word in enumerate(rowlabels):\n",
    "    spacematrix[index] = space[word]\n",
    "\n",
    "# start SVD\n",
    "# SVD = U * S * V\n",
    "# SVD finds three different takes on our source matrix: U, S and V\n",
    "# V is returned as already transposed matrix (feature of numpy), so it's important to take it into account, we will want to transpose it later for the analysis\n",
    "# full_matrices = False, not True because False would keep it orthogonal\n",
    "\n",
    "umatrix, sigmavector, vmatrixT = np.linalg.svd(spacematrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80) (80,) (80, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(umatrix.shape, sigmavector.shape, vmatrixT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25121212e+04, 4.16232798e-11, 2.24941277e-12, 2.24941277e-12])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmavector[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.25121212e+04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 4.16232798e-11, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.24941277e-12, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.24941277e-12]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(sigmavector[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmavector is a matrix of importance, a matrix of the weight of each feature\n",
    "# This matrix helps to determine which dimensions we are getting rid of and how many\n",
    "# For example, in above the first dimension seems super important, while all others have a very low score, e.g. 2.24e-12\n",
    "# So here it seems like it's ok to get rid of all dimensions except the first one. That is, reduce our matrix from size 2000 to size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U, S and VT are decomposition of original vectors\n",
    "# A proof that we can reproduce the original vector space by multiplying these matrices (checking for their mathematical properties)\n",
    "\n",
    "reconstructed = umatrix @ np.diag(sigmavector) @ vmatrixT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(reconstructed, spacematrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U and VT are orthonormal\n",
    "# Another proof of the properties of the matrices\n",
    "\n",
    "np.allclose(umatrix.transpose() @ umatrix, np.eye(umatrix.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(umatrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(vmatrixT @ vmatrixT.transpose(), np.eye(vmatrixT.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmavector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 80)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmatrixT.transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: the standard SVD algorithm (and the one that has the code above) returns V matrix which is already transposed\n",
    "# It's important to take it into account, because in line 76 (just before that comment), I transpose the matrix\n",
    "# and it's of size 2000 x 80, which means that it's 80 words (vocab of the target dataset) and each of them\n",
    "# is represented as a vector with 2000 dimensions (each dimension corresponds to the word from our source dataset, the gutenberg corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly check and try to interpret all three components of SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_V = vmatrixT.transpose()[:10, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_of_V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_part = list(new_space)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>argument</th>\n",
       "      <th>ball</th>\n",
       "      <th>beam</th>\n",
       "      <th>body</th>\n",
       "      <th>boom</th>\n",
       "      <th>bow</th>\n",
       "      <th>burn</th>\n",
       "      <th>butler</th>\n",
       "      <th>chatter</th>\n",
       "      <th>child</th>\n",
       "      <th>cigar</th>\n",
       "      <th>cigarette</th>\n",
       "      <th>click</th>\n",
       "      <th>company</th>\n",
       "      <th>concentration</th>\n",
       "      <th>courage</th>\n",
       "      <th>determination</th>\n",
       "      <th>digress</th>\n",
       "      <th>discussion</th>\n",
       "      <th>eye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.323</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.962</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.231</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.153</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.049</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   argument   ball   beam   body   boom    bow   burn  butler  chatter  child  \\\n",
       "0    -0.323  0.946  0.002 -0.001 -0.000 -0.000  0.000  -0.000   -0.001 -0.000   \n",
       "1    -0.308 -0.107  0.945  0.000 -0.000  0.000  0.000  -0.000    0.000 -0.000   \n",
       "2    -0.244 -0.084 -0.089 -0.962  0.001 -0.001  0.001  -0.001    0.000 -0.000   \n",
       "3    -0.231 -0.079 -0.084  0.074  0.963  0.002  0.003   0.001    0.003 -0.005   \n",
       "4    -0.198 -0.068 -0.072  0.063 -0.062 -0.970 -0.010   0.014   -0.007 -0.004   \n",
       "5    -0.166 -0.057 -0.061  0.053 -0.057  0.040  0.979   0.003   -0.001 -0.001   \n",
       "6    -0.153 -0.052 -0.056  0.049 -0.048  0.032 -0.037  -0.982   -0.001 -0.001   \n",
       "7    -0.147 -0.049 -0.054  0.047 -0.051  0.038 -0.038   0.035    0.982 -0.002   \n",
       "8    -0.142 -0.048 -0.052  0.044 -0.041  0.039 -0.036   0.034   -0.033  0.983   \n",
       "9    -0.136 -0.047 -0.050  0.043 -0.048  0.036 -0.035   0.034   -0.035 -0.033   \n",
       "\n",
       "   cigar  cigarette  click  company  concentration  courage  determination  \\\n",
       "0 -0.001      0.000 -0.001    0.000          0.000    0.000         -0.000   \n",
       "1 -0.000      0.000 -0.001   -0.000          0.000    0.000         -0.000   \n",
       "2 -0.000     -0.000 -0.000   -0.000          0.000    0.000          0.000   \n",
       "3 -0.004      0.000 -0.001   -0.002          0.001    0.001         -0.002   \n",
       "4  0.006     -0.006  0.003   -0.004         -0.005    0.005         -0.001   \n",
       "5  0.001     -0.001  0.000   -0.001         -0.001    0.001         -0.001   \n",
       "6 -0.000     -0.001  0.000   -0.001         -0.001    0.001         -0.000   \n",
       "7 -0.001      0.000  0.000   -0.001          0.001    0.000         -0.002   \n",
       "8  0.000      0.001  0.000    0.000          0.001    0.001          0.000   \n",
       "9 -0.983     -0.003 -0.001    0.003         -0.000   -0.000          0.006   \n",
       "\n",
       "   digress  discussion    eye  \n",
       "0   -0.001      -0.000  0.000  \n",
       "1   -0.000       0.000 -0.000  \n",
       "2   -0.000       0.000  0.000  \n",
       "3    0.002      -0.001  0.001  \n",
       "4    0.006       0.002 -0.001  \n",
       "5    0.002       0.000 -0.000  \n",
       "6    0.001       0.000 -0.000  \n",
       "7    0.001       0.001  0.001  \n",
       "8    0.001       0.000 -0.001  \n",
       "9    0.001       0.001 -0.000  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(part_of_V, columns=vocab_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_U = umatrix[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_of_U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.994</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.993</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9\n",
       "0 -0.112  0.994  0.000  0.000  0.000 -0.000  0.000  0.000 -0.000 -0.000\n",
       "1 -0.112 -0.013  0.994 -0.000 -0.000  0.000 -0.000 -0.000  0.000  0.000\n",
       "2 -0.112 -0.013 -0.013  0.994 -0.000 -0.000 -0.000  0.000 -0.000  0.000\n",
       "3 -0.112 -0.013 -0.013 -0.013  0.993 -0.000  0.000 -0.000  0.000 -0.000\n",
       "4 -0.112 -0.013 -0.013 -0.013 -0.013  0.993 -0.000  0.000 -0.000  0.000\n",
       "5 -0.112 -0.013 -0.013 -0.013 -0.013 -0.013  0.993 -0.000  0.000  0.000\n",
       "6 -0.112 -0.013 -0.013 -0.013 -0.013 -0.013 -0.013  0.993 -0.000 -0.000\n",
       "7 -0.112 -0.013 -0.013 -0.013 -0.013 -0.013 -0.013 -0.014  0.993  0.000\n",
       "8 -0.112 -0.013 -0.013 -0.013 -0.013 -0.013 -0.013 -0.014 -0.014  0.993\n",
       "9 -0.112 -0.013 -0.013 -0.013 -0.013 -0.013 -0.013 -0.014 -0.014 -0.014"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(part_of_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22512.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5     6     7     8     9\n",
       "0 22512.121 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "1     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "2     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "3     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "4     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "5     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "6     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "7     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "8     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
       "9     0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.diag(sigmavector[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16999c280>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sigmavector)\n",
    "\n",
    "# We see that sigma is high only for the first feature\n",
    "# It's nearly a 0 for all other features\n",
    "# What does it tell us about the corpus that we are using?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x169ac6c10>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sigmavector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 10\n",
    "vocab = list(new_space.keys())\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    print(top_words)\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 2000)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmatrixT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function show_topics.<locals>.<lambda> at 0x169af4dc0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ball beam heart mind prosper company flinch flare tooth optimism',\n",
       " 'beam courage fire flinch discussion chatter kick heart cigarette bow',\n",
       " 'boom burn girl flare kick fire concentration flame courage lessen',\n",
       " 'boom body chatter burn face bow digress machine fear optimism',\n",
       " 'body butler cigar digress head courage shudder roam throb gabble',\n",
       " 'burn body bow butler digress head roam courage fire cigar',\n",
       " 'body bow digress head prosper sale gabble courage roam hope',\n",
       " 'chatter body bow butler face machine head sale hope roam',\n",
       " 'child body bow butler digress concentration cigarette hope fire roam',\n",
       " 'body bow butler determination thunder shot girl throb machine fire',\n",
       " 'cigarette body bow butler cigar fire digress courage gabble rifle',\n",
       " 'click bow body butler cigar digress fire determination shudder shot',\n",
       " 'bow body butler cigar digress fire throb gabble determination rifle',\n",
       " 'bow body company cigar butler digress fire shot fear kick',\n",
       " 'body bow company cigar butler concentration fire throb machine shudder',\n",
       " 'determination body cigar bow company butler concentration courage digress stray',\n",
       " 'digress bow body company cigar butler concentration courage face hope',\n",
       " 'body bow company butler cigar concentration courage gabble fire stray',\n",
       " 'eye body bow company cigar butler concentration courage discussion fire',\n",
       " 'face body bow company cigar butler courage concentration discussion shot']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(vmatrixT.transpose()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topics above are basically dimensions which have features with the highest values for the first 20 words in our matrix\n",
    "# We can use it to basically see what are the topics that the model captures, each line is supposed to be a separate theme/topic\n",
    "# No clear topics here, but in a better space it should be possible to have a clearer picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to how we can use SVD space to construct vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrase_svd_space(phrase, svd_space):\n",
    "\n",
    "    subject_space = svd_space[phrase[0]]\n",
    "    verb_space = svd_space[phrase[1]]\n",
    "        \n",
    "    representation = np.zeros(len(svd_space))\n",
    "\n",
    "    out = subject_space + verb_space\n",
    "    #out = subject_space * verb_space\n",
    "    #out = subject_space * 0.2 + verb_space * 0.8\n",
    "    #out = subject_space * 0.0 + verb_space * 0.95 + (0.05 * subject_space * verb_space)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['face', 'glow']\n",
      "['face', 'beam']\n",
      "['face', 'burn']\n"
     ]
    }
   ],
   "source": [
    "print(reference)\n",
    "print(landmark_high)\n",
    "print(landmark_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = build_phrase_svd_space(reference, new_space)\n",
    "lhigh = build_phrase_svd_space(landmark_high, new_space)\n",
    "llow = build_phrase_svd_space(landmark_low, new_space)\n",
    "\n",
    "#print(ref)\n",
    "#print(lhigh)\n",
    "#print(llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, lhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(ref, llow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.03386333e+03, -1.04714725e-12, -5.73110421e-14, -5.80505701e-14,\n",
       "       -5.88094332e-14, -5.95884001e-14, -6.03882802e-14, -6.12099272e-14,\n",
       "       -6.20542419e-14, -6.29221756e-14, -6.38147333e-14, -6.47329781e-14,\n",
       "       -6.56780350e-14, -6.66510960e-14, -6.76534243e-14, -6.86863608e-14,\n",
       "       -6.97513291e-14, -7.08498428e-14, -7.19835123e-14, -7.31540526e-14,\n",
       "       -7.43632925e-14,  2.19278231e-12, -3.84529050e-14, -3.91217214e-14,\n",
       "       -1.30180436e-14, -1.87204564e-14, -1.27909580e-13, -4.44015678e-13,\n",
       "       -2.05828613e-13, -1.96101469e-13,  2.71800409e-13,  1.10954063e-13,\n",
       "        2.11938597e-12,  6.43451852e-14,  1.32292534e-13, -1.12583192e-13,\n",
       "       -2.28066005e-13, -5.99130851e-14,  3.65370131e-15, -3.39385192e-14,\n",
       "        2.66705745e-14,  2.48470368e-14, -1.02257789e-14,  1.27937261e-14,\n",
       "       -1.65317713e-14, -1.98378273e-14,  1.11462903e-14, -4.03585134e-15,\n",
       "       -2.34868164e-14, -6.06489718e-16])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.03386333e+03, -1.04714725e-12, -5.73110421e-14,  2.20592166e-12,\n",
       "       -2.94047166e-14, -2.97942000e-14, -3.01941401e-14, -3.06049636e-14,\n",
       "       -3.10271210e-14, -3.14610878e-14, -3.19073666e-14, -3.23664890e-14,\n",
       "       -3.28390175e-14, -3.33255480e-14, -3.38267122e-14, -3.43431804e-14,\n",
       "       -3.48756646e-14, -3.54249214e-14, -3.59917561e-14, -3.65770263e-14,\n",
       "       -3.71816462e-14,  2.23058891e-12, -1.38808954e-25,  3.71677513e-27,\n",
       "       -6.50499540e-28, -4.08315414e-27, -1.89542486e-28,  8.00110669e-28,\n",
       "       -6.73800243e-29, -1.31790062e-27,  4.41019953e-28,  5.03699965e-28,\n",
       "       -1.07089206e-27, -5.20012848e-28,  7.05115140e-28,  4.09499495e-28,\n",
       "       -1.20067530e-28,  7.33450527e-28, -3.61552823e-28, -2.99678970e-29,\n",
       "       -1.33665695e-28, -5.44670015e-28, -6.21774864e-29, -1.84392053e-29,\n",
       "       -2.15147516e-28,  9.63312760e-29, -2.96730790e-29, -5.46505228e-28,\n",
       "       -2.39684794e-29,  4.90536784e-28])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.03386333e+03, -1.04714725e-12, -5.73110421e-14, -5.80505701e-14,\n",
       "       -5.88094332e-14, -5.95884001e-14, -6.03882802e-14,  2.20355738e-12,\n",
       "       -3.10271210e-14, -3.14610878e-14, -3.19073666e-14, -3.23664890e-14,\n",
       "       -3.28390175e-14, -3.33255480e-14, -3.38267122e-14, -3.43431804e-14,\n",
       "       -3.48756646e-14, -3.54249214e-14, -3.59917561e-14, -3.65770263e-14,\n",
       "       -3.71816462e-14,  2.23058891e-12, -1.39824727e-25,  2.80601616e-27,\n",
       "       -1.28701257e-28, -1.93806524e-27,  2.30011815e-28, -2.81014806e-28,\n",
       "        1.11253448e-27, -1.28005577e-27,  7.24461606e-28,  5.56203316e-28,\n",
       "       -8.16445108e-28, -2.98147940e-28, -5.67551722e-29,  1.57836841e-28,\n",
       "       -1.32266157e-28,  7.26911506e-28, -1.83641792e-28,  7.64280925e-29,\n",
       "       -2.77035419e-28, -2.56411332e-28, -3.49301394e-28, -2.22243205e-28,\n",
       "       -4.89296937e-28,  6.85061761e-30, -5.27798679e-29, -3.61675792e-28,\n",
       "       -1.15758737e-29,  6.34050662e-30])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing for homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating composition functions VS human judgements\n",
    "\n",
    "As part of the homework, you will be asked to test how the cosine similarity between vectors of multiple vector spaces compares with the human judgements on the words collected in the previous step.\n",
    "\n",
    "For comparison of several scores we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Some guidelines for implementing the correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = {}\n",
    "j['landmark_high'] = []\n",
    "j['landmark_low'] = []\n",
    "\n",
    "for participant in dataset:\n",
    "    #if participant == 'participant8':\n",
    "    judgements = dataset[participant]\n",
    "    for ref, landmark, score, gt in judgements:\n",
    "        if ['face', 'glow'] == ref and landmark == ['face', 'beam']:\n",
    "            j['landmark_high'].append(score)\n",
    "        if ['face', 'glow'] == ref and landmark == ['face', 'burn']:\n",
    "            j['landmark_low'].append(score)\n",
    "\n",
    "#dataset['participant1'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(j['landmark_high']))\n",
    "print(len(j['landmark_low']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 6, 2, 6, 4, 5, 7, 3, 4, 6, 5, 5, 7, 6, 6, 7, 6, 6, 2, 5, 5,\n",
       "       6, 7, 6, 4, 6, 6, 7, 4, 5, 6, 5, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high = np.array([int(elem) for elem in j['landmark_high']])\n",
    "high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 3, 3, 4, 4, 5, 2, 3, 1, 1, 7, 2, 5, 4, 1, 4, 1, 6, 7, 4, 7,\n",
       "       3, 4, 7, 3])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = np.array([int(elem) for elem in j['landmark_low']])\n",
    "low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: try to correlate model's predictions with _averaged_ rankings from all humans (when available).\n",
    "\n",
    "For example, for model A (simple additive), you would have two conditions: high and low rankings.\n",
    "First, gather human judgements for your pairs.\n",
    "Average them (it might happen that single pair does not have the same number of human judgements for either high or low condition.\n",
    "For the same pairs, gather model A cosine scores.\n",
    "\n",
    "In the end, you should have two vectors of same size.\n",
    "Now, compute Spearman correlation on these two vectors and the result will be the correlation between model's predictions and human judgements for highly rated phrases.\n",
    "Similar procedure should be done for pairs with low ratings.  \n",
    "\n",
    "Question to consider: What would you do to get one single number for *all* human judgements vs. each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236–244). Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
