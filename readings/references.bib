@article{Turney:2010aa,
  author =        {Turney, Peter D and Pantel, Patrick and others},
  journal =       {Journal of artificial intelligence research},
  number =        {1},
  pages =         {141--188},
  title =         {From frequency to meaning: Vector space models of
                   semantics},
  volume =        {37},
  year =          {2010},
  doi =           {http://dx.doi.org/10.1613/jair.2934},
}

@techreport{Pulman:2005ab,
  address =       {Oxford, United Kingdom},
  author =        {Pulman, Stephen G.},
  institution =   {Department of Computer Science, University of Oxford},
  type =          {lecture notes},
  title =         {Higher Order Logic in Semantics},
  year =          {2005},
}

@unpublished{Manning:2005aa,
  author =        {Christopher D. Manning},
  note =          {Lecture notes for CS224N/Ling 280},
  title =         {An Introduction to Formal Computational Semantics},
  year =          {2005},
  url =           {https://prod-c2g.s3.amazonaws.com/cs224n/Fall2012/files/cl-
                  semantics-new.pdf},
}

@techreport{Jurafsky:2019aa,
  author =        {Jurafsky, Dan and Martin, James H.},
  institution =   {Stanford University and University of Colorado at
                   Boulder},
  month =         {October 16},
  type =          {Third edition draft},
  title =         {Speech and language processing: an introduction to
                   natural language processing, computational
                   linguistics, and speech recognition},
  year =          {2019},
  url =           {https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf},
}

@article{Erk:2012aa,
  author =        {Erk, Katrin},
  journal =       {Language and Linguistics Compass},
  number =        {10},
  pages =         {635--653},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Vector Space Models of Word Meaning and Phrase
                   Meaning: A Survey},
  volume =        {6},
  year =          {2012},
  abstract =      {Distributional models represent a word through the
                   contexts in which it has been observed. They can be
                   used to predict similarity in meaning, based on the
                   distributional hypothesis, which states that two
                   words that occur in similar contexts tend to have
                   similar meanings. Distributional approaches are often
                   implemented in vector space models. They represent a
                   word as a point in high-dimensional space, where each
                   dimension stands for a context item, and a word's
                   coordinates represent its context counts. Occurrence
                   in similar contexts then means proximity in space. In
                   this survey we look at the use of vector space models
                   to describe the meaning of words and phrases: the
                   phenomena that vector space models address, and the
                   techniques that they use to do so. Many word meaning
                   phenomena can be described in terms of semantic
                   similarity: synonymy, priming, categorization, and
                   the typicality of a predicate's arguments. But vector
                   space models can do more than just predict semantic
                   similarity. They are a very flexible tool, because
                   they can make use of all of linear algebra, with all
                   its data structures and operations. The dimensions of
                   a vector space can stand for many things: context
                   words, or non-linguistic context like images, or
                   properties of a concept. And vector space models can
                   use matrices or higher-order arrays instead of
                   vectors for representing more complex relationships.
                   Polysemy is a tough problem for distributional
                   approaches, as a representation that is learned from
                   all of a word's contexts will conflate the different
                   senses of the word. It can be addressed, using either
                   clustering or vector combination techniques. Finally,
                   we look at vector space models for phrases, which are
                   usually constructed by combining word vectors. Vector
                   space models for phrases can predict phrase
                   similarity, and some argue that they can form the
                   basis for a general-purpose representation framework
                   for natural language semantics.},
  doi =           {10.1002/lnco.362},
  issn =          {1749-818X},
  url =           {http://dx.doi.org/10.1002/lnco.362},
}

@incollection{Clark:2015aa,
  author =        {Clark, Stephen},
  booktitle =     {Handbook of Contemporary Semantics --- second
                   edition},
  chapter =       {16},
  editor =        {Lappin, Shalom and Fox, Chris},
  pages =         {493--522},
  publisher =     {Wiley -- Blackwell},
  title =         {Vector Space Models of Lexical Meaning},
  year =          {2015},
}

@book{Chierchia:2000uq,
  address =       {Cambridge, Mass},
  author =        {Chierchia, Gennaro and McConnell-Ginet, Sally},
  edition =       {2},
  publisher =     {MIT Press},
  title =         {Meaning and grammar: an introduction to semantics},
  year =          {2000},
  isbn =          {0262032694},
}

@book{BlackburnBos:2005,
  author =        {Blackburn, Patrick and Bos, Johan},
  publisher =     {CSLI Publications},
  title =         {Representation and inference for natural language.
                   {A} first course in computational semantics},
  year =          {2005},
  url =           {http://www.coli.uni-saarland.de/publikationen/softcopies/
                  Blackburn:1997:RIN.pdf},
}

@book{Bird:2009aa,
  author =        {Bird, Steven and Klein, Ewan and Loper, Edward},
  publisher =     {O'Reilly},
  title =         {Natural language processing with {P}ython},
  year =          {2009},
  abstract =      {This is an introduction to natural language
                   processing, which supports a variety of language
                   technologies, from predictive text and email
                   filtering to automatic summarization and translation},
  isbn =          {9780596516499},
  url =           {http://nltk.org/book/},
}

